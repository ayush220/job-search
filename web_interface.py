import asyncio
import json
import sys
import subprocess
from http.server import HTTPServer, BaseHTTPRequestHandler
from urllib.parse import parse_qs, urlparse
import webbrowser
import os
import threading
import pandas as pd

class JobSearchHandler(BaseHTTPRequestHandler):
    def _set_headers(self, content_type='text/html'):
        self.send_response(200)
        self.send_header('Content-type', content_type)
        self.send_header('Access-Control-Allow-Origin', '*')
        self.end_headers()
    
    def do_GET(self):
        if self.path == '/':
            # Serve the main HTML page
            self._set_headers()
            with open('web_interface.html', 'rb') as file:
                self.wfile.write(file.read())
        elif self.path.startswith('/search'):
            # Handle job search request
            self._set_headers('application/json')
            
            # Parse query parameters
            query = urlparse(self.path).query
            params = parse_qs(query)
            
            # Get search parameters
            title = params.get('title', ['software engineer,backend'])[0]
            location = params.get('location', ['all'])[0]
            
            # Build command to run job scraper
            cmd = [sys.executable, 'job_scraper.py']
            if title:
                cmd.extend(['--title', title])
            if location:
                cmd.extend(['--location', location])
            
            # Run the job scraper
            try:
                result = subprocess.run(cmd, capture_output=True, text=True, check=True)
                output = result.stdout
                
                # Read the CSV file generated by the job scraper
                if os.path.exists('jobs.csv'):
                    df = pd.read_csv('jobs.csv')
                    jobs = df.to_dict('records')
                    
                    # Send the jobs as JSON
                    self.wfile.write(json.dumps({
                        'success': True,
                        'output': output,
                        'jobs': jobs
                    }).encode())
                else:
                    self.wfile.write(json.dumps({
                        'success': False,
                        'output': output,
                        'error': 'No jobs found'
                    }).encode())
            except subprocess.CalledProcessError as e:
                self.wfile.write(json.dumps({
                    'success': False,
                    'output': e.stdout,
                    'error': e.stderr
                }).encode())
        else:
            # Serve static files
            try:
                self._set_headers()
                with open(self.path[1:], 'rb') as file:
                    self.wfile.write(file.read())
            except FileNotFoundError:
                self.send_response(404)
                self.end_headers()
                self.wfile.write(b'File not found')

def run_server(port=3000):
    server_address = ('', port)
    httpd = HTTPServer(server_address, JobSearchHandler)
    print(f"Server running at http://localhost:{port}")
    httpd.serve_forever()

def open_browser(port=3000):
    # Wait a moment for the server to start
    import time
    time.sleep(1)
    webbrowser.open(f"http://localhost:{port}")

if __name__ == '__main__':
    port = 3000
    # Start the browser in a separate thread
    threading.Thread(target=open_browser, args=(port,)).start()
    # Run the server
    run_server(port)
